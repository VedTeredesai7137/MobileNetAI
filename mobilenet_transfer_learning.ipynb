{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a63bf7f6-24c7-4968-97c3-f113ff6994f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to C:\\Users\\USER/.cache\\torch\\hub\\checkpoints\\mobilenet_v2-b0353104.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 13.6M/13.6M [00:00<00:00, 23.5MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV2(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6(inplace=True)\n",
      "    )\n",
      "    (1): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (9): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (10): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (11): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (12): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (13): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (14): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (15): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (16): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (17): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (18): Conv2dNormActivation(\n",
      "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.2, inplace=False)\n",
      "    (1): Linear(in_features=1280, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "# Load pretrained MobileNetV2\n",
    "model = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Freeze all layers initially\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modify classifier (last layer)\n",
    "num_classes = 3\n",
    "model.classifier[1] = nn.Linear(model.last_channel, num_classes)\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b870b837-d158-4c8b-8669-a4d2f0f41f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Folder structure created at D:\\VED\\Coding\\ML\\MobileNetAI\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b3c7da83c00456c800900bff71370b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading 'dog':   0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Downloaded 800 images for 'dog'\n",
      "📂 dog: 640 train | 160 val\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "217bb9befd6f4bfd95dc498cca335890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading 'cat':   0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Downloaded 800 images for 'cat'\n",
      "📂 cat: 640 train | 160 val\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4988253e3edc46a9bfdf066de4d13b5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading 'cow':   0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Downloaded 800 images for 'cow'\n",
      "📂 cow: 640 train | 160 val\n",
      "\n",
      "✅ Dataset ready in 4836.83 seconds\n"
     ]
    }
   ],
   "source": [
    "# ===============================================\n",
    "# 📥 BLOCK 1: DOWNLOAD DATASET FROM PEXELS\n",
    "# ===============================================\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "from math import floor\n",
    "\n",
    "# 🔐 Your PEXELS API key\n",
    "PEXELS_API_KEY = \"c0RqbswWMj17E5o2KbQroMPADjcMhoGX3DoKxU5IVkUZkh9DupaarqlJ\"\n",
    "\n",
    "# 📂 Dataset structure\n",
    "BASE_DIR = r\"D:\\VED\\Coding\\ML\\MobileNetAI\"\n",
    "TRAIN_DIR = os.path.join(BASE_DIR, \"train\")\n",
    "VAL_DIR = os.path.join(BASE_DIR, \"val\")\n",
    "\n",
    "# 🐶 Define your classification classes\n",
    "CLASSES = [\"dog\", \"cat\", \"cow\"]\n",
    "PER_CLASS = 800  # total per class (e.g., 640 train + 160 val)\n",
    "\n",
    "# 🔗 Pexels API\n",
    "BASE_URL = \"https://api.pexels.com/v1/search\"\n",
    "HEADERS = {\"Authorization\": PEXELS_API_KEY}\n",
    "\n",
    "# 🛠️ 1. Create folder structure\n",
    "def create_base_structure():\n",
    "    os.makedirs(TRAIN_DIR, exist_ok=True)\n",
    "    os.makedirs(VAL_DIR, exist_ok=True)\n",
    "    for cls in CLASSES:\n",
    "        os.makedirs(os.path.join(TRAIN_DIR, cls), exist_ok=True)\n",
    "        os.makedirs(os.path.join(VAL_DIR, cls), exist_ok=True)\n",
    "    print(f\"✅ Folder structure created at {BASE_DIR}\")\n",
    "\n",
    "# 📥 2. Fetch images for one class\n",
    "def fetch_images_for_class(cls, total_images=PER_CLASS):\n",
    "    temp_dir = os.path.join(BASE_DIR, f\"temp_{cls}\")\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "    count = 0\n",
    "    page = 1\n",
    "    pbar = tqdm(total=total_images, desc=f\"Downloading '{cls}'\")\n",
    "    while count < total_images:\n",
    "        params = {\"query\": cls, \"per_page\": 80, \"page\": page}\n",
    "        resp = requests.get(BASE_URL, headers=HEADERS, params=params)\n",
    "        time.sleep(0.5)\n",
    "        if resp.status_code != 200:\n",
    "            print(f\"❌ Error {resp.status_code} for {cls} page {page}\")\n",
    "            break\n",
    "\n",
    "        photos = resp.json().get(\"photos\", [])\n",
    "        if not photos:\n",
    "            print(f\"⚠️ No more photos for {cls}\")\n",
    "            break\n",
    "\n",
    "        for photo in photos:\n",
    "            if count >= total_images:\n",
    "                break\n",
    "            img_url = photo[\"src\"][\"original\"]\n",
    "            img_id = photo[\"id\"]\n",
    "            ext = img_url.split(\".\")[-1].split(\"?\")[0]\n",
    "            filename = f\"{img_id}.{ext}\"\n",
    "            save_path = os.path.join(temp_dir, filename)\n",
    "            try:\n",
    "                img_data = requests.get(img_url).content\n",
    "                with open(save_path, \"wb\") as f:\n",
    "                    f.write(img_data)\n",
    "                count += 1\n",
    "                pbar.update(1)\n",
    "            except:\n",
    "                continue\n",
    "        page += 1\n",
    "\n",
    "    pbar.close()\n",
    "    print(f\"✅ Downloaded {count} images for '{cls}'\")\n",
    "    return temp_dir\n",
    "\n",
    "# ✂️ 3. Split into train/val\n",
    "def split_images(temp_dir, cls):\n",
    "    files = os.listdir(temp_dir)\n",
    "    random.shuffle(files)\n",
    "\n",
    "    train_count = floor(len(files) * 0.8)\n",
    "    train_files = files[:train_count]\n",
    "    val_files = files[train_count:]\n",
    "\n",
    "    train_cls_dir = os.path.join(TRAIN_DIR, cls)\n",
    "    val_cls_dir = os.path.join(VAL_DIR, cls)\n",
    "\n",
    "    for f in train_files:\n",
    "        os.rename(os.path.join(temp_dir, f), os.path.join(train_cls_dir, f))\n",
    "    for f in val_files:\n",
    "        os.rename(os.path.join(temp_dir, f), os.path.join(val_cls_dir, f))\n",
    "\n",
    "    os.rmdir(temp_dir)\n",
    "    print(f\"📂 {cls}: {len(train_files)} train | {len(val_files)} val\")\n",
    "\n",
    "# 🚀 4. Main pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    create_base_structure()\n",
    "    start = time.time()\n",
    "    for cls in CLASSES:\n",
    "        tdir = fetch_images_for_class(cls, PER_CLASS)\n",
    "        split_images(tdir, cls)\n",
    "        time.sleep(2)\n",
    "    print(f\"\\n✅ Dataset ready in {round(time.time() - start, 2)} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6e16fe48-87ba-459b-a57d-32e4c810a8d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚡ Using device: cuda\n",
      "📸 Train: 1920 | Val: 480 | Classes: ['cat', 'cow', 'dog']\n",
      "🚀 Training started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch [1/10] | Loss: 0.7634 | Val Acc: 83.75% | ⏱️ 0:31:49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch [2/10] | Loss: 0.4600 | Val Acc: 90.62% | ⏱️ 1:04:07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch [3/10] | Loss: 0.3722 | Val Acc: 89.58% | ⏱️ 1:34:49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch [4/10] | Loss: 0.3572 | Val Acc: 88.96% | ⏱️ 2:07:09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch [5/10] | Loss: 0.3394 | Val Acc: 90.83% | ⏱️ 2:39:40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch [6/10] | Loss: 0.3303 | Val Acc: 90.42% | ⏱️ 3:09:15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch [7/10] | Loss: 0.3031 | Val Acc: 90.42% | ⏱️ 3:40:23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch [8/10] | Loss: 0.2976 | Val Acc: 91.46% | ⏱️ 4:12:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch [9/10] | Loss: 0.2823 | Val Acc: 88.75% | ⏱️ 4:44:09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch [10/10] | Loss: 0.2792 | Val Acc: 91.04% | ⏱️ 5:14:19\n",
      "💾 Model saved as mobilenet_best.pth\n"
     ]
    }
   ],
   "source": [
    "# ===============================================\n",
    "# 🧠 BLOCK 2: TRAINING MOBILE NET\n",
    "# ===============================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"⚡ Using device: {device}\")\n",
    "\n",
    "# Paths\n",
    "train_dir = os.path.join(BASE_DIR, \"train\")\n",
    "val_dir = os.path.join(BASE_DIR, \"val\")\n",
    "\n",
    "# Transforms\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Datasets\n",
    "train_data = datasets.ImageFolder(train_dir, transform=transform_train)\n",
    "val_data = datasets.ImageFolder(val_dir, transform=transform_val)\n",
    "class_names = train_data.classes\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"📸 Train: {len(train_data)} | Val: {len(val_data)} | Classes: {class_names}\")\n",
    "\n",
    "# Load MobileNetV2\n",
    "model = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "for p in model.features.parameters():\n",
    "    p.requires_grad = False\n",
    "model.classifier[1] = nn.Linear(model.last_channel, len(class_names))\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=0.0005)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"🚀 Training started...\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "    train_loss /= len(train_data)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (preds == labels).sum().item()\n",
    "    val_acc = 100 * correct / total\n",
    "\n",
    "    elapsed = timedelta(seconds=int(time.time() - start_time))\n",
    "    print(f\"✅ Epoch [{epoch+1}/{num_epochs}] | Loss: {train_loss:.4f} | Val Acc: {val_acc:.2f}% | ⏱️ {elapsed}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"mobilenet_best.pth\")\n",
    "print(\"💾 Model saved as mobilenet_best.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a300c8d3-3218-49cc-acc0-af989d2289f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded and ready!\n",
      "🖼️ Image: D:\\VED\\Coding\\ML\\MobileNetAI\\train\\cow\\46505.jpeg\n",
      "🔮 Predicted Class: cow (97.70% confidence)\n"
     ]
    }
   ],
   "source": [
    "# --------------------------- Imports ---------------------------\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------------------ Device Setup ------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --------------------- Load Trained Model ---------------------\n",
    "# Make sure to load your model structure before loading weights\n",
    "from torchvision import models\n",
    "num_classes = 3\n",
    "class_names = ['dog', 'cat', 'cow']\n",
    "\n",
    "model = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "model.classifier[1] = torch.nn.Linear(model.last_channel, num_classes)\n",
    "model.load_state_dict(torch.load(\"mobilenet_best.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(\"✅ Model loaded and ready!\")\n",
    "\n",
    "# --------------------- Transform Definition -------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# --------------------- Prediction Function --------------------\n",
    "def predict_image(img_path):\n",
    "    \"\"\"Predict class of a single image\"\"\"\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    img_tensor = transform(img).unsqueeze(0).to(device)  # (1, C, H, W)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(img_tensor)\n",
    "        probs = F.softmax(outputs, dim=1)\n",
    "        confidence, pred_class = torch.max(probs, 1)\n",
    "\n",
    "    predicted_label = class_names[pred_class.item()]\n",
    "    conf_percent = confidence.item() * 100\n",
    "\n",
    "    print(f\"🖼️ Image: {img_path}\")\n",
    "    print(f\"🔮 Predicted Class: {predicted_label} ({conf_percent:.2f}% confidence)\")\n",
    "    return predicted_label, conf_percent\n",
    "\n",
    "# --------------------- Show Prediction ------------------------\n",
    "def show_prediction(img_path):\n",
    "    pred_label, conf = predict_image(img_path)\n",
    "    \n",
    "\n",
    "# --------------------- Test Examples --------------------------\n",
    "# Example 1: Dog image\n",
    "show_prediction(r\"D:\\VED\\Coding\\ML\\MobileNetAI\\train\\cow\\46505.jpeg\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1aa3b0f4-a260-4db4-a46d-af78dd933201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded trained weights from mobilenet_best.pth\n",
      "\n",
      "======================================================================\n",
      "       -- CALCULATING MODEL RESOURCES FOR MOBILE DEPLOYMENT --        \n",
      "======================================================================\n",
      "+--------------------+----------------+\n",
      "| Metric             |          Value |\n",
      "+--------------------+----------------+\n",
      "| Input Resolution   |        224x224 |\n",
      "| Total MACs (MAdds) | 326.21 Million |\n",
      "| Total Parameters   |   2.23 Million |\n",
      "| Calculation Time   | 0.0275 seconds |\n",
      "+--------------------+----------------+\n",
      "======================================================================\n",
      "+-----------------------------------------------------------------------------------------------------+\n",
      "|                        FINAL PRESENTATION PROOF: MOBILE EFFICIENCY COMPARISON                       |\n",
      "+----------------------------+------------------------+------------------------+----------------------+\n",
      "| Metric                     | Your MobileNetV2 Model | Paper's MobileNet (V1) | VGG-16 (Inefficient) |\n",
      "+----------------------------+------------------------+------------------------+----------------------+\n",
      "| Computational Cost (MAdds) |     326.21 Million     |      569 Million       |    15300 Million     |\n",
      "| Parameters (Size)          |      2.23 Million      |      4.2 Million       |     138 Million      |\n",
      "+----------------------------+------------------------+------------------------+----------------------+\n",
      "\n",
      "Conclusion: Your MobileNetV2 model's computational cost (326.21 MAdds) is approximately 47x lower than the VGG-16 architecture, directly proving its viability for resource-constrained mobile or embedded devices, fulfilling the project requirements for efficiency. \n"
     ]
    }
   ],
   "source": [
    "# ===============================================\n",
    "# 🧠 BLOCK 3: CALCULATE MOBILE NET RESOURCES (CLEAN OUTPUT with PrettyTable)\n",
    "# ===============================================\n",
    "\n",
    "import torch\n",
    "# You will need to install the thop library first: pip install thop\n",
    "# And PrettyTable: pip install prettytable\n",
    "from thop import profile\n",
    "from torchvision import models\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from prettytable import PrettyTable # Import PrettyTable\n",
    "\n",
    "# --- CRITICAL FIX: DEFINE DEVICE ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- 1. Load the exact model configuration ---\n",
    "# Define placeholders needed to run this block independently\n",
    "BASE_DIR = os.getenv(\"BASE_DIR\", \".\")\n",
    "CLASSES = [\"dog\", \"cat\", \"cow\"] # Use the actual classes\n",
    "num_classes = len(CLASSES)\n",
    "input_size = (3, 224, 224) \n",
    "\n",
    "model = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "# Freeze/Modify layers exactly as done in the training block\n",
    "for p in model.features.parameters():\n",
    "    p.requires_grad = False\n",
    "model.classifier[1] = nn.Linear(model.last_channel, num_classes)\n",
    "\n",
    "# Load saved weights (Optional, but ensures the reported size is for your trained model)\n",
    "try:\n",
    "    # Replace \"mobilenet_best.pth\" with the full, correct path if running separately\n",
    "    model.load_state_dict(torch.load(\"mobilenet_best.pth\", map_location=device))\n",
    "    print(\"✅ Loaded trained weights from mobilenet_best.pth\")\n",
    "except FileNotFoundError:\n",
    "    print(\"⚠️ Weights file not found. Calculating resources for the base MobileNetV2 structure.\")\n",
    "    \n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# --- 2. Define Dummy Input ---\n",
    "dummy_input = torch.randn(1, *input_size).to(device)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"{'-- CALCULATING MODEL RESOURCES FOR MOBILE DEPLOYMENT --':^70}\")\n",
    "print(\"=\"*70)\n",
    "start_t_calc = time.time()\n",
    "\n",
    "# --- 3. Calculate MACs (MAdds) and Parameters ---\n",
    "macs, params = profile(model, inputs=(dummy_input,), verbose=False)\n",
    "\n",
    "# Convert to millions for easy reporting\n",
    "M_MACs = macs / 1_000_000\n",
    "M_Params = params / 1_000_000\n",
    "calculation_time = time.time() - start_t_calc\n",
    "\n",
    "# --- FORMATTED RESOURCE SUMMARY ---\n",
    "# We use Python's built-in formatting for the summary block\n",
    "summary = PrettyTable()\n",
    "summary.field_names = [\"Metric\", \"Value\"]\n",
    "summary.align[\"Metric\"] = \"l\"\n",
    "summary.align[\"Value\"] = \"r\"\n",
    "summary.add_row([\"Input Resolution\", f\"{input_size[1]}x{input_size[2]}\"])\n",
    "summary.add_row([\"Total MACs (MAdds)\", f\"{M_MACs:.2f} Million\"])\n",
    "summary.add_row([\"Total Parameters\", f\"{M_Params:.2f} Million\"])\n",
    "summary.add_row([\"Calculation Time\", f\"{calculation_time:.4f} seconds\"])\n",
    "\n",
    "print(summary)\n",
    "print(\"=\"*70)\n",
    "\n",
    "\n",
    "# --- 4. Final Presentation Proof Table (PrettyTable Formatting) ---\n",
    "\n",
    "# Metrics from the MobileNet papers for comparison\n",
    "paper_macs = 569   # MobileNetV1 1.0-224 (Million MAdds)\n",
    "paper_params = 4.2 # MobileNetV1 1.0-224 (Million Parameters)\n",
    "vgg_macs = 15300   # VGG-16\n",
    "vgg_params = 138\n",
    "\n",
    "# Create the final comparison table\n",
    "comparison_table = PrettyTable()\n",
    "comparison_table.title = \"FINAL PRESENTATION PROOF: MOBILE EFFICIENCY COMPARISON\"\n",
    "comparison_table.field_names = [\n",
    "    \"Metric\", \n",
    "    \"Your MobileNetV2 Model\", \n",
    "    \"Paper's MobileNet (V1)\", \n",
    "    \"VGG-16 (Inefficient)\"\n",
    "]\n",
    "\n",
    "# Set column alignment\n",
    "comparison_table.align[\"Metric\"] = \"l\"\n",
    "comparison_table.align[\"Your MobileNetV2 Model\"] = \"c\"\n",
    "comparison_table.align[\"Paper's MobileNet (V1)\"] = \"c\"\n",
    "comparison_table.align[\"VGG-16 (Inefficient)\"] = \"c\"\n",
    "\n",
    "\n",
    "# Row 1: Computational Cost (MAdds)\n",
    "comparison_table.add_row([\n",
    "    \"Computational Cost (MAdds)\", \n",
    "    f\"{M_MACs:.2f} Million\",\n",
    "    f\"{paper_macs} Million\",\n",
    "    f\"{vgg_macs} Million\"\n",
    "])\n",
    "\n",
    "# Row 2: Parameters (Size)\n",
    "comparison_table.add_row([\n",
    "    \"Parameters (Size)\", \n",
    "    f\"{M_Params:.2f} Million\",\n",
    "    f\"{paper_params} Million\",\n",
    "    f\"{vgg_params} Million\"\n",
    "])\n",
    "\n",
    "print(comparison_table)\n",
    "\n",
    "# --- CONCLUSION ---\n",
    "conclusion = (\n",
    "    f\"Conclusion: Your MobileNetV2 model's computational cost \"\n",
    "    f\"({M_MACs:.2f} MAdds) is approximately {vgg_macs / M_MACs:.0f}x lower than the \"\n",
    "    f\"VGG-16 architecture, directly proving its viability for resource-constrained \"\n",
    "    f\"mobile or embedded devices, fulfilling the project requirements for efficiency. \"\n",
    "    f\"\"\n",
    ")\n",
    "print(\"\\n\" + conclusion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0177ce7-72c5-4d1c-9dd9-eb95c4a86221",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mobilenet_pytorch]",
   "language": "python",
   "name": "conda-env-mobilenet_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
